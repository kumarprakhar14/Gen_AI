{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b448de3",
   "metadata": {},
   "source": [
    "# üéØLet's Wrap Up Phase 0 Efficiently\n",
    "> Today = Finish loose ends, \n",
    "> Tomorrow = Consolidate, \n",
    "> Then = Full speed into Generative AI! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde243c",
   "metadata": {},
   "source": [
    "## üìã Today's Compact Session (45-60 min)\n",
    "### What We'll Cover:\n",
    "\n",
    "- tf.data API (20 min) - Efficient data loading\n",
    "- TensorBoard (15 min) - Professional visualization\n",
    "- Phase 0 Summary (10 min) - What you've mastered\n",
    "- Tomorrow's Consolidation Guide (10 min) - Study plan\n",
    "\n",
    "Let's make this **practical and concise!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f1d68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPhase 0 Completion: The Essential Tools\\n- tf.data API for efficient data pipelines\\n- TensorBoard for professional visualization\\n- Best practices summary\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 0 Completion: The Essential Tools\n",
    "- tf.data API for efficient data pipelines\n",
    "- TensorBoard for professional visualization\n",
    "- Best practices summary\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8a8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07428e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PART 1: tf.data API - Efficient Data Loading\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8a6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üì¶ PART 1: tf.data API - Professional Data Pipelines\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ PART 1: tf.data API - Professional Data Pipelines\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265c8bd",
   "metadata": {},
   "source": [
    "### WHY tf.data API?\n",
    "\n",
    "Your current approach:\n",
    "```py\n",
    "  x_train = load_all_data_into_memory()  # ‚Üê 60,000 images loaded at once!\n",
    "  for batch in manual_batching(x_train):  # Manual batching\n",
    "      train(batch)\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ‚ùå All data in RAM (doesn't scale to large datasets)\n",
    "- ‚ùå No prefetching (GPU waits for CPU)\n",
    "- ‚ùå No parallelization (slow data loading)\n",
    "- ‚ùå Manual shuffling/batching code\n",
    "\n",
    "**`tf.data Solution:`**\n",
    "```py\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "  dataset = dataset.shuffle(buffer_size).batch(batch_size).prefetch(1)\n",
    "```\n",
    "**Benefits:**\n",
    "- ‚úÖ Efficient memory usage (loads on-demand)\n",
    "- ‚úÖ GPU/CPU overlap (prefetching)\n",
    "- ‚úÖ Automatic batching/shuffling\n",
    "- ‚úÖ Works with huge datasets (doesn't fit in RAM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b71d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a56fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method 1: Basic tf.data Pipeline\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Method 1: Basic tf.data Pipeline\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0328ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from tensors\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cad97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)  # Shuffle\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)     # Batch\n",
    "train_dataset = train_dataset.prefetch(1)           # Prefetch next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fe66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created tf.data pipeline:\n",
      "  - Shuffle buffer: 10000\n",
      "  - Batch size: 128\n",
      "  - Prefetching: 1 batch ahead\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úì Created tf.data pipeline:\")\n",
    "print(f\"  - Shuffle buffer: {BUFFER_SIZE}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Prefetching: 1 batch ahead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fffbd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset (no shuffling needed)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2110f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Key Concepts:\n",
      "  ‚Ä¢ shuffle(): Randomizes order (prevents overfitting)\n",
      "  ‚Ä¢ batch(): Groups samples into batches\n",
      "  ‚Ä¢ prefetch(): Loads next batch while GPU processes current batch\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüí° Key Concepts:\")\n",
    "print(\"  ‚Ä¢ shuffle(): Randomizes order (prevents overfitting)\")\n",
    "print(\"  ‚Ä¢ batch(): Groups samples into batches\")\n",
    "print(\"  ‚Ä¢ prefetch(): Loads next batch while GPU processes current batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6d37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method 2: Data Augmentation with tf.data\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Method 2: Data Augmentation with tf.data\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1bfdd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Augmentation = Artificially expand dataset by transforming images\n",
      "- Rotation: ¬±15 degrees\n",
      "- Zoom: 80-120%\n",
      "- Shift: ¬±10%\n",
      "- Flip: Horizontal (not for digits!)\n",
      "\n",
      "This prevents overfitting and improves generalization!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Data Augmentation = Artificially expand dataset by transforming images\n",
    "- Rotation: ¬±15 degrees\n",
    "- Zoom: 80-120%\n",
    "- Shift: ¬±10%\n",
    "- Flip: Horizontal (not for digits!)\n",
    "\n",
    "This prevents overfitting and improves generalization!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a02b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation function\n",
    "def augment_image(image, label):\n",
    "    \"\"\"Apply random augmentations to image\"\"\"\n",
    "    # Random rotation (-0.1 to 0.1 radians ‚âà ¬±6 degrees)\n",
    "    image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "    \n",
    "    # Random brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    \n",
    "    # Random contrast\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    \n",
    "    # Ensure values stay in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4d7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmented dataset\n",
    "train_dataset_augmented = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset_augmented = train_dataset_augmented.map(\n",
    "    augment_image, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE  # Parallel processing!\n",
    ")\n",
    "train_dataset_augmented = train_dataset_augmented.shuffle(BUFFER_SIZE)\n",
    "train_dataset_augmented = train_dataset_augmented.batch(BATCH_SIZE)\n",
    "train_dataset_augmented = train_dataset_augmented.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83f46d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created augmented dataset with:\n",
      "  - Random rotations\n",
      "  - Random brightness/contrast\n",
      "  - Parallel processing (AUTOTUNE)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úì Created augmented dataset with:\")\n",
    "print(\"  - Random rotations\")\n",
    "print(\"  - Random brightness/contrast\")\n",
    "print(\"  - Parallel processing (AUTOTUNE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a68a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∏ Visualizing Augmentation Effect:\n",
      "‚úì Saved to 'data_augmentation_example.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAAHvCAYAAAA7EDS7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAASwNJREFUeJzt3Qd81dX9P/5PAEFRQRkqDsCBYFVEXNWiYMGJWFdFquCqW2tdtW4s7r3qqBZc1PFTcdWKA/cEZy0O3CK4QRFRBO7/cY7f5J/gPTG5XJLc5Pl8PPIg+bzuOLn3vi/JO+dzTlkul8tlAAAAADR5zep7AAAAAAA0DBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUANDrvv/9+VlZWVvHx6KOPLvBt7rXXXhW3169fv6y+VP6+rr322nobBwShtiq/JkPtAQClTaMIgHrxyiuvZAcffHC29tprZ0sttVTWsmXLbNlll81++9vfZuedd1729ddf1/cQKbJPPvkkW2SRRao0FnbZZZf6Hlaj0bVr14rHdfjw4VlDFpqt5WMN4wYAGo4W9T0AAJqWOXPmZEcddVR2ySWX/Cz77LPP4scjjzySnX322dno0aOzLbfcstb30a5du+zcc8+t+HrVVVdd4HHvtttu2VprrRU/X2mllRb49pqiG264IT7/ld1zzz3ZV199FZ8zAADqn0YRAHXqsMMOy6688sqKr5dffvls1113zTp06JD997//zW677bZs7ty52RdffJENGjQoGzduXPab3/ymRrc9e/bsLJfLZW3atMmOPvrooo576623jh8U7rrrrsv7nP3rX//KDj300HoZEwAAVTn1DIA68/TTT1dpEvXu3Tt7/fXXswsvvDA74YQTsptvvjl74IEHsmbNmlU0EQ444IBs3rx5eU9ZCesGvfbaa9kOO+yQtW/fPmvVqlW8vV9ao+jLL7/MDjrooGy55ZbLFltssWz99dfP/t//+3/VrrdS3RpF868b9OCDD2abb755tsQSS2RLLrlkts0222T/+9//fvZ4hFlPYeyrr756nFETTssKp+FtuOGG2emnn57NnDlzgR/zTTfdtMrjNb8rrriiIm/btm02a9asePyDDz6Ij323bt3iY7ToootmK6ywQmzaHXnkkfFxro3x48dXeQzC91wutdZSdY/5L62NE8b/hz/8Ib4uwvOw2WabxaZjuK/K16ts/tfW888/nw0YMCBeP5wWecghh2TffvttvOytt96arbfeevGxCY9LmCX3ww8/5P0+wqyp3/3ud1mnTp3iKZZLL710PMUyzJgLjc3K8r12Q11stNFGWevWreN1f//732cfffTRzx6n8D2XO/XUU5Pf5zfffJOdeeaZ8TbDcx7G1Llz53g7+V6n5TVz4IEHxsehvGZuueWWbGEo9vPw3nvvZX/+859jLYTZgIsvvnh8rwiXD83o8Pzk891332XHHXdcfGzC63/NNdeM71/h9n5pDbTaPOfBE088ke24445xTOHy4XsNp+SF945wGqFTcQGoUzkAqCN77rln+A2p4uOhhx7Ke7khQ4ZUudyjjz5akfXt27fi+LrrrptbfPHFq1z2pZdeyr333ntVjj3yyCMV1582bVquR48eVfLyj0GDBlX5OtxOvrGHMVRW+Tq/+c1vcmVlZT+77fbt2+c+++yzKtcLx/KNo/xj7bXXzs2YMSN5X6NGjfrFx/yf//xnxeXbtGmTmzVrVpV80003rcj333//eOzTTz/NdezYsdqxXXHFFbnaOOiggyquu+KKK+buvPPOKrf36quv/uw61T3m4TlNPVfh8+WWW+5nY27WrFlu4MCBVY5VVvm1teaaa+ZatWr1s9vo169f7rzzzsv7mAwdOrTK7c2dOzceq+5x/P3vf5+bM2dOlbFXzvv06ZP3et26dat4Luevq3wf5d56661c165dk5cL3/Ott95a5fuormbmfzwrPw/VqfxYd+nSZaE+D/fcc88vPj6nnnpqlevMnj27Sm1U9z5R+f2lkOc8vA82b9682uu8/vrrNXpcAaAYnHoGQJ0JfzUvF/7C3r9//7yXGzx4cHbTTTdVuV7fvn1/drmXXnopa9GiRTZ06NA48+WNN96If/mvzoknnhgvV65Pnz5x9k+4j9TMgtp46qmnsh49emQ77bRT9vLLL2f33XdfxYyMf/7zn9lf//rXisuuuOKK8b67dOkSH4/QBwqzFcJMjTCbKJyKd/nll2d/+ctfCh5POK3vT3/6U7y9MJPk3//+d7bzzjvHLMxKefLJJysuu/fee8d/b7/99uzzzz+Pn4dxheNhZs6UKVPiY1f5eayJMMMjzIqpPKYwUyLMnpo+fXo8Fmb6nH/++VkxhNPYwsLZ5bbddts46yR87+GjJsLMmvC87L777nFGy0MPPRSPh9kj4WO11VaLr9OxY8dmEyZMiFmYLXLWWWfF0ymDc845J67LFISZJ+FxX2eddeJzHI7/+OOPcSZbr169suOPPz7vOMLzs8EGG2RbbbVVXLsrvL6CSZMmZXfeeWdcO6t8/awzzjgjmzZtWsy32GKLn63vFU7pDLNWymdfdezYMc66CrPZwvcRZvyF52rYsGHx8VpllVXy1kyoxfARxlLTx7NQxXgewntEeIzDLKjwPYdTU0M9hPGHxzQYMWJEtu+++8YZPcHFF19c5XXes2fPOEMoLMJ/9913J8dbyHP+j3/8Iz43QXjvCDPGwpg//PDD+B7y4osvLqRHFwASitJuAoAaWGyxxSr+Qt6rV6/k5cKsoMp/TT/44IPzzjYIH2FmyvxSM4p+/PHH3BJLLFFxfJNNNqn4y36YCbD55psv8IyilVZaKffNN99UZGHWU3m20047/Wys06dPz9133325K6+8Mnf++efnzj333Nxmm21WcZ3f/va3CzSjKNhrr70qrrPzzjtXHD/nnHMqjq+xxhoVxy+44IKK4wcccMDPbu/bb7/NffLJJ7mauuWWW6qMe/z48fH4PvvsU3Fs2WWXjc/Pgs4omjJlSpUZXYMHD664zvfff5/r3r173pk287+2FllkkYrbnDlzZq5FixYVWcuWLXMff/xxzN54440qt3f33XdXvJ46dOhQcfzkk0+ucl+VH/swsyxcPt9rd8MNN4yzW4Lw7zLLLFORHXnkkVVuM8zMKc9OOeWUnz0Pd911V0UeZrCE2UXlQh2EGWzl+RFHHJG3ZsJrs3ys8+bNy2255ZYLdUbRgj4Plb355pu5m2++OXfppZfG2Uih1lq3bl1xneuvv77ispVfJ2EG1nfffVeRzT+Dq/z9pdDnfPvtt684ftNNN/1s3FOnTo3fOwDUFTOKAChZYRZF+Ct/TYVZEeVrmwRhlkLz5s3j52FdpD333LNihkGhwuymsC5R5bV4wsynoHy2RxDWXQqzi8LMhbAWU8rkyZOzBRVmBJWvAxRmgMyYMSOOsfKsrfLZREFYhyjMhgh9qauuuiquL/SrX/0q6969e5yVEWZBhbViaqryGkRhBki4jSDMhBk5cmT8/NNPP42zr7bffvsF+l5feOGFKmvAhNkx5cK6NEOGDKnR1vHhMSjftj2sDRRmokydOrUiK5+tMv+OeuXP8ZtvvhkXZC/3t7/9LX7kE2abvfXWW3E2yfz++Mc/xrWrgvDvyiuvHHcGrHxfNVU+GykIM1gqrxM1vzC7KF/NhMevfA2x8BoJNRTWFVtYFvR5CMIMqjDO8u/pl2otfL/h+SsXZviENZAq10q+hdkLfc7D2knls5TCmkyh5sJzE+otfI9hzbL515kCgIXJYtYA1JmwsGu5cFpFSuVFeee/XmX5frGuTvlpTuXCYtbVfV2I8l9qKzcnylVelPuSSy6Ji1lX1yQKUgsk10ZYyDk0aILvv/8+u+OOO2IDoLyBFU5zqdxQCb+YXnDBBXFB3SCc+nLjjTdmJ510UjxlLJwyl28B33zC6WqVGwnhNKFyYXHfZZZZ5hcXtQ7mXwA49bgU6zkub0CUCwsM58vCY1dZ+XP81VdfZbVRfqpfoa+nmqjNmMrHM//jWfn5CmrTMCzEgj4PQVgw/peaRJVfU4W+hgp9zsNC26HBHJrWYQyhtsLpaGFh7l//+tfxtLfy5hgA1AUzigCoM+Ev5++++27FL1VhF6rQLJhf2Mlo/uvlE3Yvqo2wJk5l5TMzylVe16ZQ5bM/yqVmAlTeMSr8wjtmzJi4bkn4RTisSRSaSMUUZiqEtWaCMJOo/HkIQvNn/l/4wy+v+++/f/bss8/GdWLCmjj3339//DfMmgizr+Zv6OUT1mUpX38lCLu5hY98wmynMNMirIcUlM9cCcp3YysXxrEwn+P5n8fK5m9K5BPW/aksPF5hBlzK/A2h1DgWZGZJ5TGFtbzCujwpYTe0mjyeYSbYwrSgz0OY5RPWFSoX1mQK6wiFmguPZWh8zd+kK//ea/saKvQ5D9/H9ddfH9foCg2tMObwEd4TwsyosLNjmH2YbxYTACwMGkUA1JnQeKj8y86xxx4bm0WVT9UKf02v3EQJpzylGkW1FWYghVky5afShPsJW8CXn2ZVl7+IhYZIuXAqVpjFUz7jpxiLas8v/NJ68sknx5kWDz/8cDZx4sSKbJ999vnZLKAwuyE0j0Ijr7yZF2Yg9e7du2JGWOWmTkp1s4TmF2ZXhYWIw+Lb8zcpwi/OYaZHOBa2Cv/73/+e9zbCIszlz2d5U2zrrbeOn4fZGpVPt1uYwmlD4bEpf55Do+voo4/+2eVCEyKcEha2bS9mUyVs7T6/TTbZpOLz8DoL272HJuH8nnvuuYqZS/PXTHj8Qh2HJl54jMPz1ZBVrrNgl112qViwOrzX5JvJFd6PwvNXfvpZmIEXTiErn800atSooj7n4X7C5+G0usqn0oYm05FHHhk/t6A1AHVJowiAOhN+UQ2NmbAGRxB2KVpjjTXiLlgdOnSIu3zddtttFTNQwi9m4RSMyjNLFkT4y32YWXPZZZdV/KIYmiDh1KzHH3+8xqdTFUP4pbJ8Vsy9994bH5dwSkv4/ivvMFUs4XSxsBNW2B1qzpw5ccezIMyoGDhwYJXLhscirOkSdoQLz0+YfRGek/ALc7nw3IQ1Y6oTZiNV/l422mijvDNnQuOqfG2X8Et4eaMo7PZVLuzYtu6668aGWvgl++OPP857n+E0xfD9hMc0CDM1QmMp7DwVjlVee2ZhCq/Z8Ev+CSecUDFLLsziCs9BaESEWSnh9R+aMuFxDruRLajQAHn77bcrGnRhXZ1wX2H9nnD74XEJz+frr79ecUpW2J0vNGNDA/Gdd96Jz32YKRaehzDDrfy0xLD7XhDyUDPlu56F564hC6dchuei/FS0ww8/PO4kFpo5qYZPsN9++1U0eUKdbrzxxtl2220XZyfdddddRX3OL7zwwjjzLuwCGdagCg3aMOMyvHbLzT+zCwAWqjpbNhsA/m8XpUMPPbTKrkH5PsKuQGPHjv3Z9SvviBR2H8ontetZMG3atFyPHj3y3uc222xT5esPPvig1ruezb8TWep6TzzxRJUdnMo/wg5TYXe01I5Qhex6ltp9LN/OWUHYeemXnp9815tf2DGt/PLNmjWr8nhWdtJJJ1W57VdeeSUenzVrVq5bt25573/bbbdN7rYVPl9uueV+dp2wG9rWW29d5euavrYq7yg2f5Z6TsKuVkOHDv3Fx7Ly66K61+4vjfHiiy/Oe/sDBw6ssvNX2MXrl8ZU+fv46quvcquvvnrey/Xr1y/5PBRj17NiPA8HHnhg3rH3798/t8IKK+TdKS7sMLfpppvW6H3iscceW6DnvHKd5PsItTNmzJgaPa4AUAwWswagToUZCpdeemk8jemggw6KsxnCX9vD8XDqRb9+/eIaImF2w5Zbbln0+w9/mX/iiSfiDJ4wmyacYhNmm4S/3lde0Ln8sgtLmFEQZveEWVZhDGFdlG233TauUbL22msvlPsMp7XMv45K5d3OKo8trCMUZqCE2SiVn58w6yHMVgnrqVQnnNpU+RTCAQMGZJ07d8572TDLq/LaO+UzPcI6OmHGSphxFp6L8HWYlRTWbjnmmGOS9x1mLYXZTGFXtXC9MLMmzAgJayCFmTB1NUsjzDAJr6twvzvvvHOc1RVmYoXnu0uXLtmgQYOyiy66qGinwx1yyCFxR7dVVlkluX5P2E3r1VdfjTUWXntLL710PM0wPMdh0eSwy1p4fMNaPuXCZZ588sk4yya8BsprJjxPp5xyStbQhfebcOpYeMzD6XnhdRheP+EUz9TjFC4X1uQKp8eWP29hFmCY/VO+1le+11Ehz/m+++4b7yfMbAynoIXXebhO+DzsuPbYY4/F2V8AUFfin9Lq7N4AoAEIa4dU3u668volt99+e/y8W7ducftqSk84zSicXld5h6wgnD4XmiPPP/98/DqcErQwt3ancb5PhFPSyhulYf2mcBrb/K81AChl1igCoMkJMwO22mqruN5NWH8nLC4b1ga67777Ki5Tvk4OpSesZxQafWFWTFhnJ8wcC2sahZlQ5U2iwHNMdTbffPM4Oyssph9m94QdyMIso8qzgcLMRE0iABobM4oAaHLKd85KCafYhAW3F2QrcupP2B0tnC6VEp7XU089NTvppJPqdFyUltBkDItXp4RTM8MMxPId4gCgsdAoAqDJOfvss+PMgLAjV9hdKKwrEnbL+vWvfx3XCwnr8FC6Zs+eHdfqeeSRR+KuU2EmSFhzJswKCesvhVkglXdUg3yuueaaONPwtddei6eXhR+ZwxpN66+/frbHHnvENYgAoDHSKAIAAAAgsusZAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUZRA/D+++9nZWVl2csvv1zj61x77bXZUkstVe/jgKZIzUJpUbNQWtQslBY12/hoFBXRRx99lO2zzz7Z8ssvn7Vs2TLr0qVLdvjhh2dffvlltddbaaWVsqlTp2ZrrbVWje9r8ODB2VtvvVWEUUPTpWahtKhZKC1qFkqLmqWcRlGRvPvuu9n666+fTZo0Kbvpppuyt99+O7vyyiuzhx9+ONt4442zr776Ku/1Zs+enTVv3jxbbrnlshYtWtT4/hZbbLFsmWWWKeJ3AE2LmoXSomahtKhZKC1qlso0iorkkEMOiV3XBx54IOvbt2/WuXPnbJtttskeeuih7OOPP85OOOGEeLmuXbtmI0aMyIYNG5a1adMm23///fNOkbv77ruzbt26ZYsuumi2+eabZ9ddd128zPTp0/NO1Rs+fHjWq1ev7IYbboj30bZt22y33XbLZsyYUXGZ+++/P+vTp0+8Xvv27bPtttsue+edd+r0cYKGQs1CaVGzUFrULJQWNUtlGkVFELqrY8eOzQ4++ODYGa0sdFZ333337JZbbslyuVw8dt5552XrrLNO9tJLL2UnnXTSz27vvffey3bZZZdshx12yF555ZXsgAMOqCjM6oQiufPOO7N77703fjz22GPZWWedVZHPnDkzO/LII7MJEybEznCzZs2yHXfcMZs3b15RHgcoFWoWSouahdKiZqG0qFnmV/O5YSSF6XmhaNZYY428eTg+bdq07PPPP49f//a3v82OOuqoijx0YCu76qqrsu7du2fnnntu/Dp8/tprr2Wnn356teMIBRI6s0suuWT8eujQobGAyq+38847V7n8yJEjs44dO2YTJ06s1fmkUOrULJQWNQulRc1CaVGzzM+MoiIq77D+knDuZ3XefPPNbIMNNqhybMMNN/zF2w1T9MqLKujUqVP22WefVXkDGDJkSLbKKqvEaYLh8sGHH35Yo3FDY6NmobSoWSgtahZKi5qlnEZREay22mrxfMvXX389bx6OL7300rHbGSy++OILZRyLLLJIla/DmCpPwxs0aFCcVnj11Vdnzz33XPwoX4AMmhI1C6VFzUJpUbNQWtQs89MoKoKwkNYWW2yRXX755dmsWbOqZJ988kk2evTouP1feKHXRJiaF867rGz8+PELNMawpWHo7J544olZ//79K6YPQlOkZqG0qFkoLWoWSouaZX4aRUVy2WWXZT/88EO21VZbZY8//nj20UcfxVXZQ8GtsMIKv3g+ZmVhsa833ngjO/bYY7O33noru/XWW+O5mkFNi3N+oQMc3gD+8Y9/xK0Ox40bFxcCg6ZKzUJpUbNQWtQslBY1S2UaRUUStv4LXdNwvuSuu+6arbrqqnGrwLAV4DPPPJO1a9euxre18sorZ7fddlt2xx13ZD179syuuOKKilXiW7VqVdD4worwN998c/bCCy/Ehb6OOOKIisXFoClSs1Ba1CyUFjULpUXNUllZrqYrVlGvQgf3yiuvjJ1doOFTs1Ba1CyUFjULpUXNlpYW9T0A8gvnh4aV4sP0uqeeeip2Sw899ND6HhaQoGahtKhZKC1qFkqLmi1tGkUNVNj677TTTourunfu3Dk76qijsuOOO66+hwUkqFkoLWoWSouahdKiZkubU88AAAAAiCxmDQAAAECkUQQAAABApFHUwA0fPjwrKyur8tGjR4/6HhbwC/7+979nXbt2zRZddNFso402yp5//vn6HhLwC84666z4/+yf//zn+h4KkPD4449ngwYNypZffvlYr3feeWd9DwmoxowZM+L/q126dMkWW2yxbJNNNsnGjx9f38PiF2gUlYA111wzmzp1asXHk08+Wd9DAqpxyy23ZEceeWR2yimnZC+++GK2zjrrZFtttVX22Wef1ffQgITwQ+tVV12V9ezZs76HAlRj5syZ8f/V8AcZoOH74x//mD344IPZDTfckP33v//Nttxyy2zAgAHZxx9/XN9DoxoaRSWgRYsW2XLLLVfx0aFDh/oeElCNCy64INtvv/2yvffeO/vVr36VXXnllVnr1q2zkSNH1vfQgDy+/fbbbPfdd8+uvvrqbOmll67v4QDV2GabbeJOSjvuuGN9DwX4BbNmzcpuv/327Jxzzsk222yzbLXVVotnzIR/r7jiivoeHtXQKCqRrQXD9NpVVlkl/iD74Ycf1veQgITZs2dnL7zwQvxLSblmzZrFr5955pl6HRuQ3yGHHJINHDiwSt0CAAtmzpw52dy5c+NSDJWFU9CcJdOwaRQ1cGFtk2uvvTa7//77Y9f1vffeyzbddNN4rifQ8HzxxRfxP8Rll122yvHw9SeffFJv4wLyu/nmm+MpomeeeWZ9DwUAGpUll1wy23jjjbMRI0ZkU6ZMiT8j33jjjfGPp2FJFRquFvU9AH55em25sG5CaByFhcBuvfXWbN99963XsQFAKfvoo4+yww8/PK6dMP9fOwGABRfWJtpnn32yFVZYIWvevHnWu3fvbMiQIXEGPg2XGUUlZqmllspWX3317O23367voQB5hDXEwn+Cn376aZXj4euwxhjQcIQfUsMi8+GH1rAeYPh47LHHsksuuSR+Hv7yCQAUbtVVV43/t4b1AMMfaMJOwD/++GNcVoWGS6OoxIQCe+edd7JOnTrV91CAPFq2bJmtt9562cMPP1xxbN68efHrMPUWaDj69+8fd2B5+eWXKz7WX3/9uB5g+Dw0fQGABbf44ovH32GnTZuWjR07Nvvd735X30OiGk49a+COPvrobNCgQfF0s3BeZ9huO/zgGqbrAQ3TkUceme25557xF84NN9wwu+iii+J2vmEXNKBhrZ2w1lpr/ewH2fbt2//sONBw/mhaeWZ9WL8zNHbbtWuXde7cuV7HBvxcaArlcrmse/fusXaPOeaYrEePHn4ubuA0ihq4yZMnx6bQl19+mXXs2DHr06dP9uyzz8bPgYZp8ODB2eeff56dfPLJcQHrXr16xQXp51/gGgConQkTJmSbb755lT/OBOEPNGEDGKBh+frrr7Pjjjsu/l4bGro777xzdvrpp2eLLLJIfQ+NapTlQnsPAAAAgCbPGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARC2yGiorK6vpRaFRy+VyWSlQs/ATNQulRc1CaVGz0Phq1owiAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAACiFj/9A0BdWm+99ZLZoYcemsyGDRuWzK6//vpkdumllyazF198MZkBAABNixlFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARGW5XC6X1UBZWVlNLkYNNW/ePJm1bdu2qPdV3VbbrVu3Tmbdu3dPZoccckgyO++885LZkCFDktn333+fzM4666xkduqpp2Z1qYYlU+/UbP3r1atXMhs3blwya9OmTdHH8vXXXyez9u3bZ42ZmqWx6d+/fzIbPXp0Muvbt28ye/PNN7OGQs3SUJ144okF/TzarFn6b/P9+vVLZo899lhWCtQslJaa1KwZRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQtfvqHzp07J7OWLVsms0022SSZ9enTJ5kttdRSyWznnXfOGoLJkycns0suuSSZ7bjjjslsxowZyeyVV14p+e1BaZo23HDDvMdvv/325HXatm1b0JaV1dXQ7Nmzk1n79u2T2a9//etk9uKLLxZ0fzR8m222WUGvlzFjxiykEVFTG2ywQTIbP358nY4FGpu99tormR177LHJbN68eY16a3mgaTGjCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgKhF1oT06tUrmY0bN66gbaxLXXVbeZ544onJ7Ntvv01mo0ePTmZTp05NZtOmTUtmb775ZjKDYmndunUy6927dzK78cYb8x7v1KlTVmyTJk1KZuecc04yu/nmm5PZU089VdD7wJlnnpnMaPj69euXzLp165bMxowZs5BGRGXNmqX/lrfyyisnsy5duiSzsrKyBR4XNHbV1dCiiy5ap2OBhmyjjTZKZnvssUfe43379k1eZ8011yxoHEcffXQymzJlSjLr06dPrX+2D5577rmsKTCjCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgKhF1oR8+OGHyezLL79MZm3bts0aguq24ps+fXoy23zzzZPZ7Nmzk9kNN9xQi9FB6bvqqquS2ZAhQ7KGoHfv3slsiSWWSGaPPfZYQduk9+zZsxajo5QMGzYsmT3zzDN1OhZ+rlOnTslsv/32K2hL3zfeeGOBxwWNwYABA5LZYYcdVtBtVldf2223XTL79NNPC7o/qAuDBw9OZhdffHEy69ChQ97jZWVlyes8+uijyaxjx47J7Nxzz80KUd1YOlZzf7vttlvWFJhRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQNQia0K++uqrZHbMMccUtKXlSy+9lMwuueSSrBAvv/xy3uNbbLFF8jozZ85MZmuuuWYyO/zww2s5Oiht6623XjIbOHBgQVtoFrIl/T333JPMzjvvvGQ2ZcqUgt6Ppk2blsx++9vfFvX7pjQ0a+ZvRQ3ZNddcU9D1Jk2aVPSxQCnq06dPMhs1alQya9u2bUH3V90W3R988EFBtwnF0qJF+tf+9ddfP5ldffXVyax169bJ7PHHH897fMSIEcnrPPnkk8msVatWyezWW29NZltuuWVWiAkTJmRNnZ8SAQAAAIg0igAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiNL75DUxd955ZzIbN25cMpsxY0YyW2eddZLZvvvuW+utsWfOnJkV4n//+18y23///Qu6TWjIevXqlcwefPDBZNamTZtklsvlktl//vOfvMeHDBmSvE7fvn2T2YknnljQltmff/55MnvllVeS2bx585LZwIEDk1nv3r2T2YsvvpjMqDs9e/ZMZssuu2ydjoXaKXSL7ure46Ap2XPPPZPZ8ssvX9BtPvroo8ns+uuvL+g2oS7sscceBf1sWej/N4MHD857/JtvvinovlK3F2y55ZYF3ebkyZOT2XXXXZc1dWYUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAELX46R+qU+g2fl9//XVB19tvv/3yHr/lllsK2t4aGqPVV189mR1zzDEFbTn9xRdfJLOpU6fWegvNb7/9Nnmdf//73wVldW2xxRZLZkcddVQy23333RfSiKiNbbfdtqDnlrqx7LLLJrOVV165oNv8+OOPF2BEUFo6dOiQzPbZZ5+Cfm6ePn16MjvttNNqMTqoWyNGjEhmxx9/fDLL5XLJ7PLLL09mJ554YtF/f0454YQTsmL705/+lMw+//zzrKkzowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAICoxU//sDAMHz48ma233nrJrG/fvnmPDxgwIHmdBx54oJajg4avVatWyey8884raEvwGTNmJLNhw4YlswkTJiSzprrNeOfOnet7CPyC7t27F3S9//3vf0UfC7V7H1t22WWT2VtvvVXQexyUqq5du+Y9fvvttxf9vi699NJk9sgjjxT9/qA2Tj755GR2/PHHJ7PZs2cns7FjxyazY489NpnNmjUrq61FF100mW255ZYF/cxZVlaWzE477bRkdtdddyUzzCgCAAAA4P9oFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAELX46R8WhpkzZyaz/fbbL5m9+OKLeY9fffXVBW3XWd223n//+9+TWS6XS2ZQF9Zdd91ktu222xZ0m7/73e+S2WOPPVbQbUJjM378+PoeQoPTpk2bZLb11lsnsz322KOgrYCrM2LEiGQ2ffr0gm4TGrJUjfXs2bOg23v44YeT2cUXX1zQbUKxLLXUUsns4IMPLuh3t7FjxyazHXbYISu21VZbLe/x0aNHJ6+z3nrrFXRft912WzI755xzCrpNzCgCAAAA4P9oFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQ2fWsnrzzzjvJbK+99sp7fNSoUcnrDB06tKBs8cUXT2bXX399Mps6dWoyg2K54IILkllZWVlBu5fZ2eznmjVL/81g3rx5dToWGoZ27drV6f2ts846BdX6gAEDktmKK66YzFq2bJn3+O67715QncyaNSuZPffcc8nshx9+SGYtWqR/RHvhhReSGZSq6nZeOuuss2p9e08++WQy23PPPZPZ119/Xev7gmJK/R8VdOjQoaDb/NOf/pTMlllmmWS29957J7Ptt98+ma211lp5jy+xxBIF7dpWXXbjjTcWtAs51TOjCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgCi99yr1ZsyYMXmPT5o0qaBtxPv375/MzjjjjGTWpUuXZHb66acns48//jiZwfy22267ZNarV6+Ctsm8++67F3hcTcm8efMKepxffvnlhTQiiqW6bdure26vvPLKZHb88cdnxdazZ89kVlZWlszmzJmTzL777rtkNnHixLzHR44cmbzOhAkTktljjz2WzD799NNkNnny5GS22GKLJbM33ngjmUFD1rVr12R2++23F/W+3n333YLqEurb7Nmzk9nnn3+ezDp27JjM3nvvvYJ+HijUlClT8h7/5ptvktfp1KlTMvviiy+S2T333FPL0VETZhQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQtfjpH0rBa6+9lsx23XXXZDZo0KBkNmrUqGR2wAEHJLNu3bolsy222CKZQW22gG7ZsmUy++yzz5LZLbfckjVFrVq1SmbDhw8v6DbHjRuXzI477riCbpO6c/DBByezDz74IJltsskmWV368MMPk9mdd96ZzF5//fVk9uyzz2YNwf7771/QVsbVbe0NperYY49NZvPmzSvqfZ111llFvT2oK9OnT09mO+ywQzK79957k1m7du2S2TvvvJPM7rrrrmR27bXXJrOvvvoq7/Gbb745eZ1OnTols+qux8JhRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAABRi5/+oTFvo3jDDTcks2uuuSaZtWiRfnlsttlmyaxfv37J7NFHH01mUBs//PBDMps6dWrWWLVq1SqZnXjiicnsmGOOSWaTJ09OZueff34y+/bbb5MZDd/ZZ59d30NoEvr371/Q9W6//faijwXqQq9evZLZlltuWdT7qm7r7jfffLOo9wUNwXPPPZfMOnbsmDUUqd8V+/btm7zOvHnzktm7775blHFRc2YUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAEKX3P6fB6dmzZzLbZZddktkGG2yQzFq0KOwlMHHixGT2+OOPF3SbUBt333131hS3Fq5um/vBgwcXtIXwzjvvXIvRAXVhzJgx9T0EKMgDDzyQzJZeeumCbvPZZ5/Ne3yvvfYq6PaAhWuxxRbLe3zevHnJ6+RyuWR28803F2Vc1JwZRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAESF7Y3OAuvevXsyO/TQQ/Me32mnnZLXWW655bJimzt3bjKbOnVqMqtu20OYX1lZWUHZDjvskMwOP/zwrKE74ogjktlJJ52UzNq2bZvMRo8encyGDRtWi9EBQGHat29f9J8RL7/88rzHv/3224JuD1i4xo4dW99DYAGZUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAEDU4qd/KFR129IPGTIkmR166KHJrGvXrlldmTBhQjI7/fTTk9ndd9+9kEZEU5PL5QrKqqu9Sy65JJmNHDkymX355ZfJ7Ne//nUyGzp0aN7j66yzTvI6K664YjL78MMPC9puNLV9MNAwlZWVJbPVV189mT377LMLaURQM6NGjUpmzZoV/+/QTz/9dNFvE1h4ttpqq/oeAgvIjCIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAACiFj/9w7LLLpvMfvWrXyWzyy67LJn16NEjqyvPPfdcMjv33HOT2V133ZXM5s2bt8DjgoWlefPmyezggw9OZjvvvHMy++abb5JZt27dsrra6veRRx5JZieffHJRxwHUn1wuV6dbjENt9OrVK5kNGDCgoJ8fZ8+encz+/ve/J7NPP/00mQENzyqrrFLfQ2AB+SkEAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACAqEXWyLRr1y6ZXXXVVQVtAVrX2/ults0+//zzk9cZO3ZsMps1a1ZRxgULwzPPPJPMxo8fn8w22GCDgu5vueWWS2bLLrtsQbf55Zdf5j1+8803J69z+OGHF3RfQNOw8cYbJ7Nrr722TsdC07TUUksV9H9pdT7++ONkdvTRRxd0m0DD88QTT+Q93qxZep7KvHnzFuKIqC0zigAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiDSKAAAAAIhaZA3URhttlMyOOeaYZLbhhhsmsxVWWCGrS999910yu+SSS5LZGWeckff4zJkzizIuaEgmT56czHbaaadkdsABBySzE088MSu2iy++OJldccUVeY+//fbbRR8H0HiUlZXV9xAAoOhee+21vMcnTZqUvM4qq6ySzFZdddVk9vnnn9dydNSEGUUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABELbIGascddywoK9TEiROT2b333pvM5syZk8zOP//8ZDZ9+vRajA6apqlTpyaz4cOHF5QB1KX//Oc/yez3v/99nY4FauONN95IZk8//XQy69Onz0IaEVDqzjjjjGR2zTXXJLPTTz89mR122GEF/Y5P9cwoAgAAACDSKAIAAAAg0igCAAAAINIoAgAAACDSKAIAAAAg0igCAAAAICrL5XK5rAbKyspqcjFo9GpYMvVOzcJP1CyUFjULpUXNUlNt2rRJZrfeemsyGzBgQDK74447ktnee++dzGbOnJk1Vbka1KwZRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAERluRruZ2g7QfiJLUChtKhZKC1qFkqLmqUY2rRpk8xOP/30ZHbQQQcls549eyaziRMnZk1VrgY1a0YRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAUVmuhvsZ2k4QfmILUCgtahZKi5qF0qJmofHVrBlFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARGW5UtnPEAAAAICFyowiFkjXrl2ziy66qL6HAdSQmoXSomahtKhZKC1qNj+NooRnnnkma968eTZw4MCssVEMNEZqFkqLmoXSomahtKhZFoRGUcI///nP7LDDDssef/zxbMqUKfU9HOAXqFkoLWoWSouahdKiZlkQGkV5fPvtt9ktt9ySHXTQQbEDe+2111Zk4fOlllqqyuXvvPPOrKysrMqx0047LVtmmWWyJZdcMvvjH/+Y/fWvf8169epVke+1117ZDjvskJ1xxhnZsssuG2/zb3/7WzZnzpzsmGOOydq1a5etuOKK2ahRo6rc7kcffZTtuuuu8fLhMr/73e+y999//2e3e95552WdOnXK2rdvnx1yyCHZjz/+GPN+/fplH3zwQXbEEUfEMVce95NPPpltuumm2WKLLZattNJK2Z/+9Kds5syZFflnn32WDRo0KOYrr7xyNnr06KI83rCg1KyapbSoWTVLaVGzapbSombV7ILSKMrj1ltvzXr06JF1794922OPPbKRI0dmtVnzO7zgTj/99Ozss8/OXnjhhaxz587ZFVdc8bPLjRs3LnZ3Q5f3ggsuyE455ZRsu+22y5Zeeunsueeeyw488MDsgAMOyCZPnhwvH4pjq622isX6xBNPZE899VS2xBJLZFtvvXU2e/bsitt95JFHsnfeeSf+e91118U3g/I3hzvuuCMWbCjiqVOnxo8gXD7czs4775y9+uqr8Y0lFNqhhx5apWhDYYfbve2227LLL788FhvUNzWrZiktalbNUlrUrJqltKhZNbvAwq5nVLXJJpvkLrroovj5jz/+mOvQoUPukUceiV+PGjUq17Zt2yqXHzNmTKi6iq832mij3CGHHFLlMr/5zW9y66yzTsXXe+65Z65Lly65uXPnVhzr3r17btNNN634es6cObnFF188d9NNN8Wvb7jhhniZefPmVVzmhx9+yC222GK5sWPHVrndcN1yv//973ODBw+u+DrkF154YZXx7bvvvrn999+/yrEnnngi16xZs9ysWbNyb775Zvwen3/++Yr89ddfj8fmvy2oa2r2J2qWUqFmf6JmKRVq9idqllKhZn+iZgtnRtF83nzzzez555/PhgwZEr9u0aJFNnjw4HiOZ21uY8MNN6xybP6vgzXXXDNr1uz/fwrClL2111674uuw+FiYalfe5XzllVeyt99+O3ZgQ+c1fITpet9//33soFa+3XDdcmHK3i91SsNthy5t+e2Gj9DtnTdvXvbee+9lr7/+enws1ltvvYrrhC71/NMWoa6pWTVLaVGzapbSombVLKVFzarZYmhRlFtpREIBhfMql19++YpjYZpeq1atsssuuywWwvzT9srPl6ytRRZZpMrX4fzKfMfCi7v8XNPwws53LmXHjh2rvd3y20gJtx2mBYbzOOcXphq+9dZbNfyuoG6pWTVLaVGzapbSombVLKVFzarZYtAoqiQU1PXXX5+df/752ZZbblklCwtq3XTTTVmXLl2yGTNmxEWxFl988Zi9/PLLVS4bzgUdP358NmzYsIpj4esF1bt373iuZVhUrE2bNgXfTsuWLbO5c+f+7LYnTpyYrbbaanmvE7qt4fEJ56husMEGFZ3m6dOnFzwOWFBqVs1SWtSsmqW0qFk1S2lRs2q2WJx6Vsm9996bTZs2Ldt3332ztdZaq8pHWBQrdGc32mijrHXr1tnxxx8fp8f961//qrKKfBC2IQyXDQtvTZo0Ka4YHxbUmn8l+drafffdsw4dOsSV4cPiX2EK3aOPPhq7puULhNVE165d44JjH3/8cfbFF1/EY8cee2z29NNPx8W+whtFGPddd91VsfhXeLMIi4OFLm1YmCwUWFj9PqwYD/VFzapZSouaVbOUFjWrZiktalbNFotGUSWhGAYMGJC1bdv2Z1korAkTJsQX8I033pjdd9998fzL0JUdPnz4zwrguOOOy44++ujY2QwFEFZYX3TRRRdofKGgQ0GEqXM77bRTtsYaa8Q3gXBOZ206smGF+LAF4aqrrloxxa9nz57ZY489FqfkhS0F11133ezkk0+uMmUxbG0Yvu7bt2+8//333z92g6G+qFk1S2lRs2qW0qJm1SylRc2q2WIpCytaF+3WSNpiiy2y5ZZbLrvhhhvqeyhADahZKC1qFkqLmoXSomabFmsULQTfffddduWVV8ZV1sNq7aFL+9BDD2UPPvhgfQ8NyEPNQmlRs1Ba1CyUFjWLGUULwaxZs7JBgwZlL730UpxGF86HPPHEE+P0NqDhUbNQWtQslBY1C6VFzaJRBAAAAEBkMWsAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAohZZDZWVldX0otColcr672oWfqJmobSoWSgtahYaX82aUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABA1OKnf2iqjjvuuGTWvHnzZLbLLrsks7XXXjsrtuHDhyezESNGFP3+AAAAoCkyowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAICoxU//0Ji3uT/++OOTWevWrbO69NFHHyWzJ554IpnddNNNC2lEUD969OiRzI466qhkts8++ySzkSNHJrPzzz8/mb3xxhvJDAAAaFrMKAIAAAAg0igCAAAAINIoAgAAACDSKAIAAAAg0igCAAAAINIoAgAAACAqy+VyuawGysrKanIxiqBfv355j5988snJ6/Tt2zerS5MmTUpm48ePT2ZDhw7NSl0NS6beqdn6t/rqqyezZ599Npm1adOm6GP5+uuvk1n79u2zxkzN1t6aa65Z9O/htddeW4AR0ZSoWSgtahYaX82aUQQAAABApFEEAAAAQKRRBAAAAECkUQQAAABApFEEAAAAQKRRBAAAAEBUlqvhfoa2E6y9zTffPJkNHz48mfXp0yerK9VtV7znnnsms1deeaXkt8gsVKl8f2q27qy11lp5j993333J66ywwgoFvcZmzJiRzH744Ydk1qFDh2S23nrrFVTrpULNFvcxe+aZZ5LZyiuvnMzGjx+fzF5//fVkds899ySzyZMnJ7Ovv/46mU2bNi2ZUf/UbO0NGDCg6I/nU089lcy+//77gm6TxknNNk3NmqXnnLRq1arWP6vOmzevKOOiODVrRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAABRWa6G+xk25e0Eq9uu/swzz0xmm2yySVZX3nnnnWR27rnnJrN//vOfycwWhfnZArRpqm4L+dtuuy3v8ZVWWqmg56e619iECROS2WmnnZbM7rzzzoLGcuyxxyaz8847LysFarb2ttlmm4L+b+vWrVsyGzx4cFaXpk2blsz+8pe/5D3+7LPPJq/z2muvFWVc/DI12zAes5VXXjmZvf/++0W/P0qXmm281lprrWT2wAMPJLNlllkm7/HPP/88eZ2LLroomZ199tnJjIVTs2YUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAEJXlarifYWPfTnDMmDHJbODAgcmsefPmRR/LlClT8h6/6aabCtoy8MsvvyzKuPiJLUAbr1GjRiWzoUOH1tnzU+hrbI899khm++yzTzLr379/MvvXv/5VZ4/JwqJm606zZum/P62wwgrJ7LbbbktmSy+9dDJbbbXVGsTjOWPGjGQ2evToZFbd/+vVWWONNQp6Dvr27ZsV22677Vb021SzDeMxmzx5cjL729/+lsyuvvrqoo+Fhk3NNl6fffZZMmvXrl2dPT/Dhw9PZiNGjCjqOJqCXA1q1owiAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAorJcDfczbOzbCY4fPz6Z9e7du6DbvPfee5PZU089lcxuvPHGvMenTJlS0DgoLluAlrYePXoks6effjqZtW3bttb3NW7cuGR25513JrNLLrkkmVX3PrD55psns1VXXTWZ3X///bV+PwqGDh2alQI123h16tQpmf3jH/9IZtttt91CGlHTszBet2q29s4444xkdtxxx9XpWF544YVkdsIJJySzsWPHLqQRsbCp2cZr7ty5BV2vQ4cOeY9fffXVyesMHDgwmbVs2TKZ/eEPf0hmt9xySzJrynI1qFkzigAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiDSKAAAAAIjKcjXcz7AxbCfYs2fPZDZ+/Phk1qJFi2Q2Z86cZNaqVatajI5SYQvQhm/11VdPZs8++2wya9OmTUH3d++99+Y9vsMOOxS0lX3v3r2T2RVXXJHMvvvuu6zYW59Wd5sbbLBBMnvjjTeyhkLNMr8nnngi7/FmzdJ/P+vXr18yO+SQQ5LZtttum5W6MWPGFPSeVCg1W1wHHnhgMlt55ZWT2V/+8pc6fW7POeecZHbWWWflPT59+vSijIsFo2Ybr9dffz2ZdevWLZktueSSeY/PmjUreZ3bbrstmVX3M3V1qvuZ+tVXX82aqlwNataMIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAAKKyXA33M2zs2wn++9//TmZbb711Mqvu4dtpp52S2d13312L0dGQ2AK0YVhhhRWS2RlnnJHM9thjj2T2+eefJ7MpU6Yks5NPPjnv8XvvvTcrBXPnzi3o9T569Ohktueee2YNhZplfsOGDct7/Lrrrktep0uXLsnsww8/LMq4+ImabRi22WabZHbHHXcks0UXXbToY3n22WfzHt9xxx2T1/nkk0+KPg7yU7ON14gRI5LZcccdl8yOPvrovMcvuuiigsZx3nnnJbM///nPyey7774r6HeCuxv57+o1qVkzigAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiDSKAAAAAIjKcjXcz7Cxbye4yCKLJLMzzzwzmR1xxBEF3d99992XzPbff/+8x6dOnVrQfVFctgBtGMaMGZPMBg0alMy+/fbbZLb99tsns+effz6ZLbbYYnmPT5s2LSsFc+fOLej1/uSTTyazfv36ZQ2FmmV+bdu2zXt8+vTpyevst99+yeyaa64pyrj4iZpt+DbffPNa/xwb7LbbbkUdx6uvvprMJk2alMxOPvnkZDZx4sQFHldTo2Ybr86dOyezd999N5k9/fTTeY9vttlmWbHdc889yWybbbZJZp9//nky22KLLZLZa6+9ljWFmjWjCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgKgsV8P9DJvydoLNmzcvaAvQyy67rKD7++abb/Ie32mnnZLXeeSRRwq6L2rPFqB1Z5111klmL7zwQkG3uckmmySz559/PmuK5s6dW9Dr/cknn0xm/fr1yxoKNUsxXisjR45MZvvuu+9CGlHTpGYbr44dOyaz5557LpmtvPLKWV158MEHk9nOO++czGbMmJE1VWq29rp27ZrMNt9882Q2adKkgn4u69OnTzKbNWtWMnvjjTeS2cSJE5PZUkstlfd49+7dk9f55JNPsrqs5+oe50suuSSZHXnkkVlTqFkzigAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiOx6thANGzYsmW288cbJbOutt857fOmll05ep3PnzrXeRY3C2Nmh7jzxxBMF7V42bty4ZLbFFlss8Lia0mt63rx5yeypp55KZptttlnWUKhZamr06NHJbIMNNkhmq6+++kIaUdOkZpumVVZZJZltv/32eY9feOGFWV167bXXktn555+fzB599NGCdnr6/vvvs1LQlGt28ODByezUU09NZtXt/lXdz14LQ7NmzQr6mbpbt27JbIUVVqj17obV/WxfqA033DCZPfPMM8nsv//9bzLr1atXVursegYAAABAjWkUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQleVquJ+hLUDrzvrrr1/r7QQffPDBZLb11lsXZVz8pClvAbow9O/fP5ndc889yaxly5bJ7LDDDktmV1xxRS1G1zTMnTu3oNf7pZdemsyOOOKIrKFQs9TUyJEjk9nee++dzDx3xaVmqaldd901md1yyy1ZKbjyyiuT2UEHHZSVgsZes9X9PLrNNtsUfSx1/Xg2lLFU9zvrQw89VPT7e+CBBwr6/aR58+ZZqavJ82pGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAFGLn/6hIZkwYULe4w8++GDyOltssUUyGz58eEEZ1IXWrVsns5YtWyazTz/9NJmNHj16gcfV2Jx66qkFXa+6950jjjhiAUYEDc+HH35Y0PX69OmTzJ588skFGBFQnVtvvbXWP0//0v+J2223XTJbaqmlsmI78MADk9ngwYOLfn/t2rUr+m02Bueff34y22abbZLZuHHjktmRRx6ZzN5+++2soRg6dGgyW2211Qq6XseOHfMeLysrS17nrrvuSmbvv/9+Mjv55JOT2e23357MlllmmWSGGUUAAAAA/B+NIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAorJcLpfLaqC6reyoG9VtzXjvvfcms8suuyyZHX744Qs8rqamhiVT70qlZgcNGpTMxowZU9A21qusskrWFJ100knJ7JRTTklmkydPTmbDhg1LZo8//nhWCtQsNbXhhhsms+eeey6ZHXbYYQX9H0x+apb6tP766xf0PtCsWWn8/X1hvG4bQ83OmTMnmd16663J7K9//WtBP6s2dv379897fK+99kpe5w9/+EPRX2P33XdfMtt2220L+n/9iiuuyEpdTR7P0nhHAwAAAGCh0ygCAAAAINIoAgAAACDSKAIAAAAg0igCAAAAINIoAgAAACBq8dM/FOriiy9OZmeccUYy+/TTT2t9X8OHD88KceihhyazCRMmFLRN8O23357MHn300VqMDgpzxx13ZI3V6quvnsxOOOGEZLb77rsXVLODBw+uxeig8Zo7d25B16uuhi677LIFGBFQ16r72bh58+YF3eZtt92WzAYOHJjMFl100YLu79VXXy3oek3ZKqusksya8jb3hXr44YdrdTyYOHFiMhsxYkRB49h2220Lut7HH3+cNXVmFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABCV5XK5XFYDZWVlNblYkzN9+vRk1qpVq2R2//33J7N27drlPd6nT5+soZg0aVIy69GjR9aY1bBk6l2p1Oz222+fzMaMGZPMPvroo2TWtWvXrKE75JBDCtoCtG3btsnsuuuuS2b77LNP1lSpWYqhuvec999/P5ltuummC2lEjZeapSlZf/31k9nSSy9d0G3+73//S2ZTpkzJik3NsrBtscUWyezqq69OZiuttFJB93fxxRcnsyOPPDIrdTWpWTOKAAAAAIg0igAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiFpkJah9+/YFbSNZ6PZ4Sy65ZDJbfPHFk1mzZs0K2hK8FHTs2DGZde7cOZl9+OGHC2lENMbtGavLOnXqlMzOP//8ZHbVVVclsy+++CKZ/eY3v6n11vPrrrtuQe9HH3zwQTL797//ncwuuuiiZAYANDwTJkyo7yFAg/fggw8msx49eiSzP/7xj8lso402SmYjR47MmjozigAAAACINIoAAAAAiDSKAAAAAIg0igAAAACINIoAAAAAiDSKAAAAAIjKctXtP11JWVlZVpd69uxZ0BbQ6623XjJbYoklktm//vWvZNanT5+CtoKvS6NHj05m06ZNS2bjxo1LZo8++mgymz17djKbNWtW1pjVsGTqXV3XbKEGDRqUzMaMGVP0+5s6dWoy++abb5JZ9+7dizqOp59+Opk98MADyey0004r6jiaAjVLMXz00UfJ7P33309mm2666UIaUeOlZqG0qFlofDVrRhEAAAAAkUYRAAAAAJFGEQAAAACRRhEAAAAAkUYRAAAAAJFGEQAAAABRi6yB2mKLLZLZxhtvnMwWXXTRZDZq1Khktv/++yezG2+8MZmNHDkymX377bfJ7MILL0xm0JQ8+eSTyey5555LZhtttFFB97f88ssns06dOhV0m1988UWt3zuOPvrogu4L6tuQIUOS2R/+8IesoXj55ZeT2dNPP533eIcOHZLXWXHFFZPZ+++/X8vRAQA0XGYUAQAAABBpFAEAAAAQaRQBAAAAEGkUAQAAABBpFAEAAAAQaRQBAAAAEJXlcrlcVgNlZWU1uRg0ejUsmXrXGGq2TZs2yeyQQw5JZiNGjCjocanuuT3vvPOS2SWXXJL3+JQpU5LXoe6o2eIaMGBAMttqq62S2QEHHJDMmjdvnsxat26dNXSnnXZaMjvppJPqdCyNgZqF0qJmofHVrBlFAAAAAEQaRQAAAABEGkUAAAAARBpFAAAAAEQaRQAAAABEGkUAAAAARGW5Gu5naDtB+IktQKG0qNnS1rt372S24447JrO11lqrqOP47rvvktnuu+9e1Ptq6tQslBY1C42vZs0oAgAAACDSKAIAAAAg0igCAAAAINIoAgAAACDSKAIAAAAg0igCAAAAICrL1XA/Q9sJwk9sAQqlRc1CaVGzUFrULDS+mjWjCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACASKMIAAAAgEijCAAAAIBIowgAAACAqCyXy+V++hQAAACApsyMIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAACKNIgAAAAAijSIAAAAAIo0iAAAAALLg/wNPCkSLYhfd2gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize augmentation\n",
    "print(\"\\nüì∏ Visualizing Augmentation Effect:\")\n",
    "sample_images = x_train[:5]\n",
    "sample_labels = y_train[:5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Original vs Augmented Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(5):\n",
    "    # Original\n",
    "    axes[0, i].imshow(sample_images[i, :, :, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Original\\n{np.argmax(sample_labels[i])}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Augmented\n",
    "    aug_img, _ = augment_image(sample_images[i], sample_labels[i])\n",
    "    axes[1, i].imshow(aug_img.numpy()[:, :, 0], cmap='gray')\n",
    "    axes[1, i].set_title('Augmented', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_augmentation_example.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úì Saved to 'data_augmentation_example.png'\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d810a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method 3: Using tf.data with Model Training\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Method 3: Using tf.data with Model Training\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc548e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\Desktop\\KP Programming\\AI\\Gen_AI\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build simple CNN\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    keras.layers.MaxPooling2D((2,2)),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # Regularization\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da376c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be0fbcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model ready\n",
      "\n",
      "üöÄ Training with tf.data pipeline:\n",
      "(This is much cleaner than manual batching!)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úì Model ready\")\n",
    "print(\"\\nüöÄ Training with tf.data pipeline:\")\n",
    "print(\"(This is much cleaner than manual batching!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce366f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 46ms/step - accuracy: 0.7427 - loss: 0.7822 - val_accuracy: 0.9159 - val_loss: 0.2652\n",
      "Epoch 2/3\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 70ms/step - accuracy: 0.8904 - loss: 0.3515 - val_accuracy: 0.9500 - val_loss: 0.1605\n",
      "Epoch 3/3\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.9159 - loss: 0.2753 - val_accuracy: 0.9639 - val_loss: 0.1201\n"
     ]
    }
   ],
   "source": [
    "# Train with tf.data (commented to save time - uncomment to run)\n",
    "history = model.fit(\n",
    "    train_dataset_augmented,  # ‚Üê tf.data dataset!\n",
    "    epochs=3,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f20fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Notice how clean the training code is!\n",
      "  No manual batching, no manual shuffling - tf.data handles everything.\n",
      "\n",
      "Comparison:\n",
      "  Manual:  for i in range(0, len(x), batch_size): ...\n",
      "  tf.data: model.fit(dataset, epochs=3)  ‚Üê That's it!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚úì Notice how clean the training code is!\n",
    "  No manual batching, no manual shuffling - tf.data handles everything.\n",
    "\n",
    "Comparison:\n",
    "  Manual:  for i in range(0, len(x), batch_size): ...\n",
    "  tf.data: model.fit(dataset, epochs=3)  ‚Üê That's it!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60e00310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PART 2: TensorBoard - Professional Visualization\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc1e9484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä PART 2: TensorBoard - Professional ML Visualization\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PART 2: TensorBoard - Professional ML Visualization\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be223687",
   "metadata": {},
   "source": [
    "### WHY TensorBoard?\n",
    "\n",
    "Your current visualization:\n",
    "  - matplotlib plots (static)\n",
    "  - Manual tracking of metrics\n",
    "  - No real-time monitoring\n",
    "  - Hard to compare experiments\n",
    "\n",
    "TensorBoard gives you:\n",
    "- ‚úÖ Real-time training monitoring\n",
    "- ‚úÖ Interactive graphs (zoom, pan, compare)\n",
    "- ‚úÖ Model architecture visualization\n",
    "- ‚úÖ Histogram of weights/gradients\n",
    "- ‚úÖ Embedding projections (t-SNE, PCA)\n",
    "- ‚úÖ Compare multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2583ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Created log directory: logs/fit/20251008-191041\n"
     ]
    }
   ],
   "source": [
    "# Create log directory\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Created log directory: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a660a6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì TensorBoard callback configured\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard callback\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,  # Log weight histograms every epoch\n",
    "    write_graph=True,  # Visualize model architecture\n",
    "    write_images=True, # Log model weights as images\n",
    "    update_freq='epoch',\n",
    "    profile_batch='500,520'  # Profile performance\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì TensorBoard callback configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482cdaa",
   "metadata": {},
   "source": [
    "### üìà TensorBoard Features:\n",
    "\n",
    "1. SCALARS: Track metrics over time\n",
    "   - Training loss, accuracy\n",
    "   - Validation loss, accuracy\n",
    "   - Learning rate\n",
    "   - Custom metrics\n",
    "\n",
    "2. GRAPHS: Model architecture visualization\n",
    "   - See your model as a computational graph\n",
    "   - Understand layer connections\n",
    "\n",
    "3. DISTRIBUTIONS: Weight/gradient histograms\n",
    "   - Monitor if weights are learning\n",
    "   - Detect vanishing/exploding gradients\n",
    "\n",
    "4. IMAGES: Visualize predictions\n",
    "   - See what model gets wrong\n",
    "   - Track improvement over epochs\n",
    "\n",
    "5. PROJECTOR: Embedding visualization\n",
    "   - t-SNE/PCA of learned representations\n",
    "   - See how model clusters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfc6fc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ To use TensorBoard:\n",
      "1. Train model with callback:\n",
      "   history = model.fit(dataset, callbacks=[tensorboard_callback])\n",
      "\n",
      "2. Launch TensorBoard:\n",
      "   tensorboard --logdir=logs/fit\n",
      "\n",
      "3. Open in browser:\n",
      "   http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "# Train with TensorBoard (commented - uncomment to run)\n",
    "print(\"\\nüöÄ To use TensorBoard:\")\n",
    "print(\"1. Train model with callback:\")\n",
    "print(\"   history = model.fit(dataset, callbacks=[tensorboard_callback])\")\n",
    "print(\"\\n2. Launch TensorBoard:\")\n",
    "print(\"   tensorboard --logdir=logs/fit\")\n",
    "print(\"\\n3. Open in browser:\")\n",
    "print(\"   http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "846611db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Demo: Custom Logging to TensorBoard\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simulate training for demo\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Demo: Custom Logging to TensorBoard\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab0fa256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created summary writers\n"
     ]
    }
   ],
   "source": [
    "# Create summary writers\n",
    "train_log_dir = log_dir + '/train'\n",
    "test_log_dir = log_dir + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "print(\"‚úì Created summary writers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "471a1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Custom Logging Example:\n",
      "   with train_summary_writer.as_default():\n",
      "       tf.summary.scalar('loss', loss_value, step=epoch)\n",
      "       tf.summary.scalar('accuracy', acc_value, step=epoch)\n"
     ]
    }
   ],
   "source": [
    "# Simulate logging (in real training, this happens automatically)\n",
    "print(\"\\nüí° Custom Logging Example:\")\n",
    "print(\"   with train_summary_writer.as_default():\")\n",
    "print(\"       tf.summary.scalar('loss', loss_value, step=epoch)\")\n",
    "print(\"       tf.summary.scalar('accuracy', acc_value, step=epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c512a5f",
   "metadata": {},
   "source": [
    "\n",
    "### Advanced TensorBoard Usage:\n",
    "\n",
    "1. Compare Multiple Runs:\n",
    "   - Train with different hyperparameters\n",
    "   - Each run saves to different directory\n",
    "   - TensorBoard overlays all runs!\n",
    "\n",
    "2. Monitor Training in Real-Time:\n",
    "   - Start TensorBoard before training\n",
    "   - Watch metrics update live\n",
    "   - Stop training early if overfitting\n",
    "\n",
    "3. Debug Model:\n",
    "   - Check weight histograms\n",
    "   - Ensure gradients are flowing\n",
    "   - Visualize layer outputs\n",
    "\n",
    "4. Share Results:\n",
    "   - TensorBoard logs are portable\n",
    "   - Share with team/advisor\n",
    "   - Include in reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8905244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PART 3: Best Practices Summary\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbdf3e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ PART 3: Best Practices for Production ML\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PART 3: Best Practices for Production ML\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84b5f944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA PIPELINE:\n",
      "‚úì Use tf.data for efficient loading\n",
      "‚úì Apply augmentation for small datasets\n",
      "‚úì Prefetch for GPU/CPU overlap\n",
      "‚úì Use AUTOTUNE for automatic optimization\n",
      "\n",
      "TRAINING:\n",
      "‚úì Use callbacks (EarlyStopping, ModelCheckpoint, TensorBoard)\n",
      "‚úì Monitor validation metrics (not just training)\n",
      "‚úì Save best model, not last model\n",
      "‚úì Use learning rate schedules\n",
      "\n",
      "MODEL ARCHITECTURE:\n",
      "‚úì Start simple, add complexity if needed\n",
      "‚úì Use Dropout for regularization\n",
      "‚úì Batch normalization for deep networks\n",
      "‚úì Residual connections for very deep networks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_practices = \"\"\"\n",
    "DATA PIPELINE:\n",
    "‚úì Use tf.data for efficient loading\n",
    "‚úì Apply augmentation for small datasets\n",
    "‚úì Prefetch for GPU/CPU overlap\n",
    "‚úì Use AUTOTUNE for automatic optimization\n",
    "\n",
    "TRAINING:\n",
    "‚úì Use callbacks (EarlyStopping, ModelCheckpoint, TensorBoard)\n",
    "‚úì Monitor validation metrics (not just training)\n",
    "‚úì Save best model, not last model\n",
    "‚úì Use learning rate schedules\n",
    "\n",
    "MODEL ARCHITECTURE:\n",
    "‚úì Start simple, add complexity if needed\n",
    "‚úì Use Dropout for regularization\n",
    "‚úì Batch normalization for deep networks\n",
    "‚úì Residual connections for very deep networks\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "122b5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PART 4: Useful Callbacks Reference\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "795bd689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîß PART 4: Essential Keras Callbacks\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß PART 4: Essential Keras Callbacks\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b084a4",
   "metadata": {},
   "source": [
    "\n",
    "1. **EarlyStopping** - Stop training when no improvement\n",
    "   ```py\n",
    "   early_stop = keras.callbacks.EarlyStopping(\n",
    "       monitor='val_loss',\n",
    "       patience=3,  # Stop if no improvement for 3 epochs\n",
    "       restore_best_weights=True\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **ModelCheckpoint** - Save best model\n",
    "   ```py\n",
    "   checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "       'best_model.h5',\n",
    "       monitor='val_accuracy',\n",
    "       save_best_only=True,\n",
    "       verbose=1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **ReduceLROnPlateau** - Reduce learning rate when stuck\n",
    "   ```py\n",
    "   reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "       monitor='val_loss',\n",
    "       factor=0.5,  # Multiply LR by 0.5\n",
    "       patience=2,\n",
    "       min_lr=1e-7\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **TensorBoard** - Visualization\n",
    "   ```py\n",
    "   tensorboard = keras.callbacks.TensorBoard(\n",
    "       log_dir='logs',\n",
    "       histogram_freq=1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **Custom Callback** - Your own logic\n",
    "   ```py\n",
    "   class CustomCallback(keras.callbacks.Callback):\n",
    "       def on_epoch_end(self, epoch, logs=None):\n",
    "           # Your custom code here\n",
    "           pass\n",
    "   ```\n",
    "\n",
    "**Usage:**\n",
    "   ```py\n",
    "   callbacks = [early_stop, checkpoint, reduce_lr, tensorboard]\n",
    "   model.fit(dataset, epochs=50, callbacks=callbacks)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "960a7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PART 5: Complete Example (Commented)\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "705fd643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù PART 5: Complete Production Pipeline Template\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù PART 5: Complete Production Pipeline Template\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51c52a",
   "metadata": {},
   "source": [
    "#### COMPLETE TEMPLATE - Copy this for future projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4eb7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\Desktop\\KP Programming\\AI\\Gen_AI\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5761 - loss: 1.2308"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 56ms/step - accuracy: 0.7389 - loss: 0.7947 - val_accuracy: 0.9266 - val_loss: 0.2412 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8822 - loss: 0.3828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 250ms/step - accuracy: 0.8936 - loss: 0.3478 - val_accuracy: 0.9571 - val_loss: 0.1458 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9158 - loss: 0.2799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 104ms/step - accuracy: 0.9208 - loss: 0.2632 - val_accuracy: 0.9586 - val_loss: 0.1301 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9282 - loss: 0.2323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 99ms/step - accuracy: 0.9316 - loss: 0.2237 - val_accuracy: 0.9666 - val_loss: 0.1078 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9392 - loss: 0.2041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 94ms/step - accuracy: 0.9412 - loss: 0.1976 - val_accuracy: 0.9712 - val_loss: 0.0886 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9439 - loss: 0.1843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 85ms/step - accuracy: 0.9456 - loss: 0.1802 - val_accuracy: 0.9740 - val_loss: 0.0837 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9495 - loss: 0.1669"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 94ms/step - accuracy: 0.9501 - loss: 0.1649 - val_accuracy: 0.9772 - val_loss: 0.0765 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 91ms/step - accuracy: 0.9522 - loss: 0.1557 - val_accuracy: 0.9754 - val_loss: 0.0775 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9547 - loss: 0.1489"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 93ms/step - accuracy: 0.9556 - loss: 0.1459 - val_accuracy: 0.9780 - val_loss: 0.0683 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9576 - loss: 0.1379"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 95ms/step - accuracy: 0.9590 - loss: 0.1355 - val_accuracy: 0.9780 - val_loss: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 91ms/step - accuracy: 0.9606 - loss: 0.1301 - val_accuracy: 0.9778 - val_loss: 0.0672 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 96ms/step - accuracy: 0.9627 - loss: 0.1247 - val_accuracy: 0.9783 - val_loss: 0.0691 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 94ms/step - accuracy: 0.9636 - loss: 0.1214 - val_accuracy: 0.9789 - val_loss: 0.0675 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9666 - loss: 0.1096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 95ms/step - accuracy: 0.9679 - loss: 0.1042 - val_accuracy: 0.9791 - val_loss: 0.0617 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9691 - loss: 0.1019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 92ms/step - accuracy: 0.9699 - loss: 0.0996 - val_accuracy: 0.9809 - val_loss: 0.0601 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9714 - loss: 0.0957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 91ms/step - accuracy: 0.9710 - loss: 0.0955 - val_accuracy: 0.9833 - val_loss: 0.0544 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 48ms/step - accuracy: 0.9711 - loss: 0.0963 - val_accuracy: 0.9817 - val_loss: 0.0592 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9721 - loss: 0.0917 - val_accuracy: 0.9839 - val_loss: 0.0570 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9724 - loss: 0.0911 - val_accuracy: 0.9833 - val_loss: 0.0580 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9758 - loss: 0.0825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.9754 - loss: 0.0818 - val_accuracy: 0.9846 - val_loss: 0.0504 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.9765 - loss: 0.0779 - val_accuracy: 0.9837 - val_loss: 0.0540 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.9768 - loss: 0.0786 - val_accuracy: 0.9845 - val_loss: 0.0520 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.9762 - loss: 0.0781 - val_accuracy: 0.9846 - val_loss: 0.0511 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9776 - loss: 0.0745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9775 - loss: 0.0751 - val_accuracy: 0.9846 - val_loss: 0.0496 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.9784 - loss: 0.0720 - val_accuracy: 0.9845 - val_loss: 0.0502 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.9790 - loss: 0.0711 - val_accuracy: 0.9849 - val_loss: 0.0503 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.9784 - loss: 0.0711 - val_accuracy: 0.9854 - val_loss: 0.0510 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 37ms/step - accuracy: 0.9792 - loss: 0.0688 - val_accuracy: 0.9855 - val_loss: 0.0497 - learning_rate: 6.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9791 - loss: 0.0675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.9794 - loss: 0.0671 - val_accuracy: 0.9861 - val_loss: 0.0486 - learning_rate: 6.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9797 - loss: 0.0666 - val_accuracy: 0.9853 - val_loss: 0.0490 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.9800 - loss: 0.0669 - val_accuracy: 0.9850 - val_loss: 0.0497 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.9796 - loss: 0.0675 - val_accuracy: 0.9855 - val_loss: 0.0493 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.9809 - loss: 0.0641 - val_accuracy: 0.9855 - val_loss: 0.0493 - learning_rate: 3.1250e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9795 - loss: 0.0682"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9797 - loss: 0.0662 - val_accuracy: 0.9852 - val_loss: 0.0482 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.9806 - loss: 0.0650 - val_accuracy: 0.9856 - val_loss: 0.0484 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.9798 - loss: 0.0656 - val_accuracy: 0.9857 - val_loss: 0.0482 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 41ms/step - accuracy: 0.9804 - loss: 0.0645 - val_accuracy: 0.9855 - val_loss: 0.0488 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9806 - loss: 0.0645 - val_accuracy: 0.9857 - val_loss: 0.0491 - learning_rate: 1.5625e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.9797 - loss: 0.0648 - val_accuracy: 0.9858 - val_loss: 0.0484 - learning_rate: 1.5625e-05\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9852 - loss: 0.0482\n",
      "Test accuracy: 0.9852\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. CREATE DATA PIPELINE\n",
    "def create_dataset(x, y, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.batch(128)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(x_train, y_train, augment=True)\n",
    "val_ds = create_dataset(x_test, y_test, augment=False)\n",
    "\n",
    "# 2. BUILD MODEL\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    keras.layers.MaxPooling2D((2,2)),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # Regularization\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. SETUP CALLBACKS\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
    "    keras.callbacks.TensorBoard(log_dir=f'logs/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "]\n",
    "\n",
    "# 4. TRAIN\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# 5. EVALUATE\n",
    "test_loss, test_acc = model.evaluate(val_ds)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# 6. VISUALIZE\n",
    "# Launch TensorBoard: tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f1a75ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚ú® Congratulations! Phase 0 Foundation is SOLID! ‚ú®\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ú® Congratulations! Phase 0 Foundation is SOLID! ‚ú®\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gen_AI (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
